{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Mercari Price Suggestion Challenge\n",
    "\n",
    "https://www.kaggle.com/c/mercari-price-suggestion-challenge\n",
    "需要下载数据文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from time import time\n",
    "import datetime\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n",
    "#Source: https://www.kaggle.com/marknagelberg/rmsle-function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "import random\n",
    "\n",
    "def get_data(dir):\n",
    "    t=time()\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train=pd.read_csv(dir+\"train.tsv\",sep='\\t')\n",
    "    train=train.sample(n=10000)  #内存小,随机选取部分数据\n",
    "#     x_train,x_test,y_train,y_test=train_test_split(data,data[\"price\"],test_size=0.2,random_state=13)\n",
    "#     train=x_train.join(x_test,rsuffix=\"_%s_\" %(text_columns[i]))\n",
    "#     test=y_train.join(y_test,rsuffix=\"_%s_\" %(text_columns[i]))\n",
    "    train[\"name\"]=train[\"name\"].fillna(\" \")+\" \"+train[\"category_name\"].fillna(\" \")+\" \"+train[\"brand_name\"].fillna(\" \")\n",
    "    train[\"item_description\"]=train[\"item_description\"].fillna(\" \")+\" \"+train[\"name\"].fillna(\" \")\n",
    "    train.drop(['category_name', 'brand_name'], axis=1)\n",
    "    train,test,y_train,y_test=train_test_split(train,train[\"price\"],test_size=0.2,random_state=13)\n",
    "  \n",
    "#     train=pd.read_csv(dir+\"train.tsv\",sep='\\t').fillna(\" \")\n",
    "#     test=pd.read_csv(dir+\"test.tsv\",sep='\\t').fillna(\" \")\n",
    "#     print(train.shape)\n",
    "#     #print(train.head(3))\n",
    "#     print(train.describe())\n",
    "#     print(test.shape)\n",
    "#     print(test.head(3))\n",
    "#     print(test.describe())\n",
    "\n",
    "\n",
    "#     train.loc[train['price'] <=0] = 1\n",
    "#     test.loc[test['price'] <=0] = 1\n",
    "#     print(train.describe())\n",
    "#     print(test.describe())\n",
    "    print(\" {0} 秒完数据读入\".format(time() - t))\n",
    " \n",
    "    t=time()\n",
    "\n",
    "    x_train=train[[\"item_condition_id\",\"shipping\"]]\n",
    "    x_test=test[[\"item_condition_id\",\"shipping\"]]\n",
    "    y_train=train[\"price\"]\n",
    "    y_test=test[\"price\"]\n",
    "    \n",
    "    x_train= pd.DataFrame(SimpleImputer ().fit_transform(x_train))\n",
    "#     y_train= Imputer().fit_transform(y_train)\n",
    "    x_test= pd.DataFrame(SimpleImputer ().fit_transform(x_test))\n",
    "#     y_test= Imputer().fit_transform(y_test)    \n",
    "    t = time()\n",
    "    print(x_train.shape)\n",
    "    #print(x_train.head(3))\n",
    "    \n",
    "    x_train,x_test=add_tfidf(x_train,x_test,train,test)\n",
    "#     x_train,x_test=add_word2vec(x_train,x_test,train,test)\n",
    "    x_train= pd.DataFrame(SimpleImputer ().fit_transform(x_train))\n",
    "    x_test= pd.DataFrame(SimpleImputer ().fit_transform(x_test))\n",
    "    x_train = pd.DataFrame(MinMaxScaler().fit_transform(x_train))\n",
    "    x_test = pd.DataFrame(MinMaxScaler().fit_transform(x_test))\n",
    "\n",
    "    # print(x_train.head(3))\n",
    "    print(x_train.shape)\n",
    "    # print(x_test.head(3))\n",
    "#     print(x_test.shape)\n",
    "    print(\" {0} 秒完成特征提取\".format(time() - t))\n",
    "    t = time()\n",
    "\n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "def get_data1(dir):\n",
    "    t=time()\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    data=pd.read_csv(dir+\"train.tsv\",sep='\\t').fillna(\" \")\n",
    "#     x_train,x_test,y_train,y_test=train_test_split(data,data[\"price\"],test_size=0.2,random_state=13)\n",
    "#     train=x_train.join(x_test,rsuffix=\"_%s_\" %(text_columns[i]))\n",
    "#     test=y_train.join(y_test,rsuffix=\"_%s_\" %(text_columns[i]))\n",
    "    \n",
    "    train,test,y_train,y_test=train_test_split(data,data[\"price\"],test_size=0.2,random_state=13)\n",
    "  \n",
    "#     train=pd.read_csv(dir+\"train.tsv\",sep='\\t').fillna(\" \")\n",
    "#     test=pd.read_csv(dir+\"test.tsv\",sep='\\t').fillna(\" \")\n",
    "#     print(train.shape)\n",
    "#     #print(train.head(3))\n",
    "#     print(train.describe())\n",
    "#     print(test.shape)\n",
    "#     print(test.head(3))\n",
    "#     print(test.describe())\n",
    "\n",
    "\n",
    "#     train.loc[train['price'] <=0] = 1\n",
    "#     test.loc[test['price'] <=0] = 1\n",
    "#     print(train.describe())\n",
    "#     print(test.describe())\n",
    "    print(\" {0} 秒完数据读入\".format(time() - t))\n",
    " \n",
    "    t=time()\n",
    "    #此时归一化，tfidf会自私归一化，而且耗时很长\n",
    "    x_train=train[[\"item_condition_id\",\"shipping\"]]\n",
    "    x_test=test[[\"item_condition_id\",\"shipping\"]]\n",
    "    y_train=train[\"price\"]\n",
    "    y_test=test[\"price\"]\n",
    "    \n",
    "    x_train= pd.DataFrame(SimpleImputer ().fit_transform(x_train))\n",
    "#     y_train= Imputer().fit_transform(y_train)\n",
    "    x_test= pd.DataFrame(SimpleImputer ().fit_transform(x_test))\n",
    "#     y_test= Imputer().fit_transform(y_test)    \n",
    "    t = time()\n",
    "    print(x_train.shape)\n",
    "    #print(x_train.head(3))\n",
    "    \n",
    "    x_train,x_test=add_tfidf2(x_train,x_test,train,test)\n",
    "#     x_train,x_test=add_word2vec(x_train,x_test,train,test)\n",
    "    x_train= pd.DataFrame(SimpleImputer ().fit_transform(x_train))\n",
    "    x_test= pd.DataFrame(SimpleImputer ().fit_transform(x_test))\n",
    "    x_train = pd.DataFrame(MinMaxScaler().fit_transform(x_train))\n",
    "    x_test = pd.DataFrame(MinMaxScaler().fit_transform(x_test))\n",
    "\n",
    "    # print(x_train.head(3))\n",
    "    print(x_train.shape)\n",
    "    # print(x_test.head(3))\n",
    "#     print(x_test.shape)\n",
    "    print(\" {0} 秒完成特征提取\".format(time() - t))\n",
    "    t = time()\n",
    "\n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "def get_data2(dir):\n",
    "    t=time()\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "    train=pd.read_csv(dir+\"train.tsv\",sep='\\t').fillna(\" \")\n",
    "    test=pd.read_csv(dir+\"test.tsv\",sep='\\t').fillna(\" \")\n",
    "    print(train.shape)\n",
    "    #print(train.head(3))\n",
    "#     print(train.describe())\n",
    "    print(test.shape)\n",
    "    #print(test.head(3))\n",
    "#     print(test.describe())\n",
    "\n",
    "\n",
    "#     train.loc[train['price'] <=0] = 1\n",
    "#     test.loc[test['price'] <=0] = 1\n",
    "#     print(train.describe())\n",
    "#     print(test.describe())\n",
    "    print(\" {0} 秒完数据读入\".format(time() - t))\n",
    " \n",
    "    t=time()\n",
    "    #此时归一化，tfidf会自私归一化，而且耗时很长\n",
    "    x_train=train[[\"item_condition_id\",\"shipping\"]]\n",
    "    x_test=test[[\"item_condition_id\",\"shipping\"]]\n",
    "    y_train=train[\"price\"]\n",
    "    y_test=test[\"price\"]\n",
    "    \n",
    "    x_train= pd.DataFrame(SimpleImputer ().fit_transform(x_train))\n",
    "#     y_train= Imputer().fit_transform(y_train)\n",
    "    x_test= pd.DataFrame(SimpleImputer ().fit_transform(x_test))\n",
    "#     y_test= Imputer().fit_transform(y_test)    \n",
    "    t = time()\n",
    "    print(x_train.shape)\n",
    "    #print(x_train.head(3))\n",
    "    \n",
    "#     x_train,x_test=add_tfidf(x_train,x_test,train,test)\n",
    "    x_train,x_test=add_word2vec(x_train,x_test,train,test)\n",
    "    x_train= pd.DataFrame(SimpleImputer ().fit_transform(x_train))\n",
    "#     y_train= Imputer().fit_transform(y_train)\n",
    "    x_test= pd.DataFrame(SimpleImputer ().fit_transform(x_test))\n",
    "#     y_test= Imputer().fit_transform(y_test)\n",
    "\n",
    "    x_train = pd.DataFrame(MinMaxScaler().fit_transform(x_train))\n",
    "    x_test = pd.DataFrame(MinMaxScaler().fit_transform(x_test))\n",
    "\n",
    "    # print(x_train.head(3))\n",
    "    print(x_train.shape)\n",
    "    # print(x_test.head(3))\n",
    "    print(x_test.shape)\n",
    "    print(\" {0} 秒完成特征提取\".format(time() - t))\n",
    "    t = time()\n",
    "\n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-trained word vectors from https://nlp.stanford.edu/projects/glove/\n",
    "# 用法https://medium.com/@japneet121/word-vectorization-using-glove-76919685ee0b\n",
    "# 安装https://github.com/maciejkula/glove-python/wiki/Installation-on-Windows\n",
    "\n",
    "def line2vec(line):\n",
    "    words=str(line).lower().split()\n",
    "    M=[]\n",
    "    for word in words:\n",
    "        try:\n",
    "            M.append()\n",
    "\n",
    "def add_glove(x_train,x_test,train,tes):\n",
    "    text_columns=[\"category_name\",\"brand_name\",\"name\",\"item_description\"]\n",
    "    max_features=[1000,10000,20000,100000]\n",
    "    max_features=[100,1000,2000,10000]\n",
    "    tfidfer=[None]*4\n",
    "    for i in range(4):\n",
    "        series=train[text_columns[i]].astype(str)\n",
    "        print(type(series))\n",
    "#         print(series.head(2))\n",
    "        embeddings_index={}\n",
    "        lines=series.values.tolist()\n",
    "        for line in lines:\n",
    "            words=line.split()\n",
    "            coefs=np.asarray(words[1:],dtype='float64')\n",
    "            edbeddings_index[word]=coefs\n",
    "        print('Found %s word vectors.' % len(embeddings_index))\n",
    "        \n",
    "        embedding_matrix=np.zeros(len(embeddings_index)) \n",
    "#         model=Word2Vec(sentences,size=num_feature)\n",
    "        b=pd.DataFrame(getAvgFeatureVecs(sentences,model,num_feature))\n",
    "        \n",
    "        x_train=x_train.join(b,rsuffix=\"_%s_\" %(text_columns[i]))\n",
    "        \n",
    "        series=test[text_columns[i]].astype(str)\n",
    "        lines=series.values.tolist()\n",
    "        sentences=[line.split() for line in lines]\n",
    "\n",
    "        \n",
    "        d=pd.DataFrame(getAvgFeatureVecs(sentences,model,num_feature))\n",
    "        x_test=x_test.join(d,rsuffix=\"_%s_\"%(text_columns[i]))\n",
    "\n",
    "        print(x_train.shape)\n",
    "#         print(x_train.head(3))\n",
    "        print(x_test.shape)\n",
    "#         print(x_test.head(3))\n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def add_tfidf(x_train,x_test,train,test):\n",
    "# 不可四列一起训练，name band 太稀少了  不划算\n",
    "    text_columns=[\"name\",\"item_description\"]\n",
    "    max_features=[100,1000]\n",
    "    ngrams=[1,1]\n",
    "    tfidfer=[None]*2\n",
    "    for i in range(len(tfidfer)):\n",
    "        tfidfer[i]=TfidfVectorizer(max_features=max_features[i],ngram_range=(1,ngrams[i]))#.fit_transform(train[text_columns[i]])\n",
    "        a=tfidfer[i].fit_transform(train[text_columns[i]])\n",
    "        b=pd.DataFrame(a.toarray(),columns=tfidfer[i].get_feature_names())\n",
    "        # print(tfidfer[i].get_feature_names())\n",
    "        print(b.shape)\n",
    "     #   print(b.head(3))\n",
    "        x_train=x_train.join(b,rsuffix=\"_%s_\" %(text_columns[i]))\n",
    "        del a,b\n",
    "        c=tfidfer[i].transform(test[text_columns[i]])\n",
    "        d=pd.DataFrame(c.toarray(),columns=tfidfer[i].get_feature_names())\n",
    "        x_test=x_test.join(d,rsuffix=\"_%s_\"%(text_columns[i]))\n",
    "\n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def add_tfidf2(x_train,x_test,train,test):\n",
    "# 不可四列一起训练，name band 太稀少了  不划算\n",
    "    text_columns=[\"category_name\",\"brand_name\",\"name\",\"item_description\"]\n",
    "    max_features=[1000,10000,20000,100000]\n",
    "    max_features=[100,1000,2000,10000]\n",
    "    tfidfer=[None]*4\n",
    "    for i in range(4):\n",
    "        tfidfer[i]=TfidfVectorizer(max_features=max_features[i],ngram_range=(1,1)).fit(train[text_columns[i]])\n",
    "        a=tfidfer[i].transform(train[text_columns[i]])\n",
    "        b=pd.DataFrame(a.toarray(),columns=tfidfer[i].get_feature_names())\n",
    "        # print(tfidfer[i].get_feature_names())\n",
    "        print(b.shape)\n",
    "     #   print(b.head(3))\n",
    "        x_train=x_train.join(b,rsuffix=\"_%s_\" %(text_columns[i]))\n",
    "\n",
    "        c=tfidfer[i].transform(test[text_columns[i]])\n",
    "        d=pd.DataFrame(c.toarray(),columns=tfidfer[i].get_feature_names())\n",
    "        x_test=x_test.join(d,rsuffix=\"_%s_\"%(text_columns[i]))\n",
    "\n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def add_word2vec(x_train,x_test,train,test):\n",
    "# 不可四列一起训练，name band 太稀少了  不划算\n",
    "    text_columns=[\"category_name\",\"brand_name\",\"name\",\"item_description\"]\n",
    "    num_features=[50,50,100,100]\n",
    "    for i in range(4):\n",
    "        series=train[text_columns[i]].astype(str)\n",
    "        print(type(series))\n",
    "#         print(series.head(2))\n",
    "        lines=series.values.tolist()\n",
    "        print(type(lines[0]))\n",
    "#         print(lines[0:3])\n",
    "#             cutWords_list = [k.split() for k in file.readlines()]\n",
    "#         sentences=[line.split() for line in lines]\n",
    "        sentences=lines\n",
    "        print(type(sentences))\n",
    "        # 模型参数\n",
    "        num_feature = num_features[i]    # Word vector dimensionality                      \n",
    "        min_word_count = 40   # Minimum word count                        \n",
    "        num_workers = 4       # Number of threads to run in parallel\n",
    "        context = 10          # Context window size                                                                                    \n",
    "        downsampling = 1e-3   # Downsample setting for frequent words\n",
    "        model = Word2Vec(sentences, workers=num_workers, \\\n",
    "                    size=num_feature, min_count=min_word_count, \\\n",
    "                    window=context, sample=downsampling)\n",
    "#         model=Word2Vec(sentences,size=num_feature)\n",
    "        b=pd.DataFrame(getAvgFeatureVecs(sentences,model,num_feature))\n",
    "        \n",
    "        x_train=x_train.join(b,rsuffix=\"_%s_\" %(text_columns[i]))\n",
    "        \n",
    "        series=test[text_columns[i]].astype(str)\n",
    "        lines=series.values.tolist()\n",
    "        sentences=[line.split() for line in lines]\n",
    "\n",
    "        \n",
    "        d=pd.DataFrame(getAvgFeatureVecs(sentences,model,num_feature))\n",
    "        x_test=x_test.join(d,rsuffix=\"_%s_\"%(text_columns[i]))\n",
    "\n",
    "        print(x_train.shape)\n",
    "#         print(x_train.head(3))\n",
    "        print(x_test.shape)\n",
    "#         print(x_test.head(3))\n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(sentence, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in sentence:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(sentences, model, num_feature):\n",
    "    counter = 0\n",
    "    vecs = np.zeros((len(sentences), num_feature), dtype=\"float64\")\n",
    "\n",
    "    for sentence in sentences:\n",
    "#         if counter % 10000 == 0:\n",
    "#             print(\"sentence %d of %d\" % (counter, len(sentences)))\n",
    "\n",
    "        vecs[counter] = makeFeatureVec(sentence, model, num_feature)\n",
    "        counter += 1\n",
    "\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ml_predict(gen_models,x_train,y_train,x_test,y_test):\n",
    "\n",
    "    models=gen_models()\n",
    "    result=[[\"model\",\"rmsle\"],[\"time\"]]\n",
    "    for model in models:\n",
    "        t=time()\n",
    "        print()\n",
    "        print(datetime.datetime.now())        \n",
    "        model=model.fit(x_train,y_train)\n",
    "        y_predict=model.predict(x_test)\n",
    "        y_predict[y_predict<=0] = 1\n",
    "        error=rmsle(y_test,y_predict)\n",
    "        print(datetime.datetime.now())\n",
    "        print(model.__module__+\"训练结束\")\n",
    "        print(\"rmsle:%f\" %error)\n",
    "#         print(\"训练结束\"+model.get_params)\n",
    "        eclapsed=time() - t\n",
    "        print(\" {0} 秒耗时总计\".format(eclapsed))\n",
    "#         result.append([model.__name__,error,eclapsed])\n",
    "        result.append([\"name\",error,eclapsed])\n",
    "        \n",
    "#         import matplotlib as mpl\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         axis=np.arange(len(y_test))\n",
    "#         mpl.rcParams['font.sans-serif'] = ['simHei']\n",
    "#         mpl.rcParams['axes.unicode_minus'] = False\n",
    "#         plt.figure(facecolor='w')\n",
    "#         plt.plot(t, y_test, 'r-', lw=2, label='真实值')\n",
    "#         plt.plot(t, y_predict, 'g-', lw=2, label='估计值')\n",
    "#         plt.legend(loc='best')\n",
    "# #         plt.title(model.___name_, fontsize=18)\n",
    "#         plt.xlabel('商品编号', fontsize=15)\n",
    "#         plt.ylabel('商品价格', fontsize=15)\n",
    "#         plt.grid()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dl_predict(model,x_train,y_train,x_test,y_test):\n",
    "\n",
    "    result=[[\"model\",\"rmsle\"],[\"time\"]]\n",
    "\n",
    "    t=time()\n",
    "    print()\n",
    "    print(datetime.datetime.now())        \n",
    "    y_predict=model(x_train,y_train,x_test)\n",
    "    y_predict[y_predict<=0] = 1\n",
    "    error=rmsle(y_test,y_predict)\n",
    "    print(datetime.datetime.now())\n",
    "    print(model.__module__+\"训练结束\")\n",
    "    print(\"rmsle:%f\" %error)\n",
    "#         print(\"训练结束\"+model.get_params)\n",
    "    eclapsed=time() - t\n",
    "    print(\" {0} 秒耗时总计\".format(eclapsed))\n",
    "#         result.append([model.__name__,error,eclapsed])\n",
    "    result.append([\"name\",error,eclapsed])\n",
    "\n",
    "#         import matplotlib as mpl\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         axis=np.arange(len(y_test))\n",
    "#         mpl.rcParams['font.sans-serif'] = ['simHei']\n",
    "#         mpl.rcParams['axes.unicode_minus'] = False\n",
    "#         plt.figure(facecolor='w')\n",
    "#         plt.plot(t, y_test, 'r-', lw=2, label='真实值')\n",
    "#         plt.plot(t, y_predict, 'g-', lw=2, label='估计值')\n",
    "#         plt.legend(loc='best')\n",
    "# #         plt.title(model.___name_, fontsize=18)\n",
    "#         plt.xlabel('商品编号', fontsize=15)\n",
    "#         plt.ylabel('商品价格', fontsize=15)\n",
    "#         plt.grid()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "模型\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "def gen_linear():\n",
    "    linear=LinearRegression()\n",
    "    ridge=Ridge()\n",
    "    lasso=Lasso()\n",
    "#     linear_pipeline=Pipeline([\n",
    "#             ('poly', PolynomialFeatures()),\n",
    "#             ('linear', ElasticNetCV(alphas=np.logspace(-3, 2, 10), l1_ratio=[.1, .5, .7, .9, .95, .99, 1],\n",
    "#                                     fit_intercept=False))])\n",
    "#     return [linear,ridge,lasso,linear_pipeline]\n",
    "    return [linear,ridge,lasso]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "def gen_tree():\n",
    "    dtr=DecisionTreeRegressor(criterion='mse', max_depth=5)\n",
    "    return [dtr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor,RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def gen_ensem():\n",
    "    ridge = RidgeCV(alphas=np.logspace(-3, 2, 3), fit_intercept=False)\n",
    "    ridged = Pipeline([('poly', PolynomialFeatures(degree=2)), ('Ridge', ridge)])\n",
    "    bagr= BaggingRegressor(ridged, n_estimators=10, max_samples=0.3)\n",
    "    rfr = RandomForestRegressor(n_estimators=10, criterion='entropy', max_depth=5, oob_score=True)\n",
    "    dbr= GradientBoostingRegressor(n_estimators=10, learning_rate=0.1, max_depth=2)\n",
    "    base_estimator = DecisionTreeRegressor(criterion='gini', max_depth=3, min_samples_split=4)\n",
    "    adar = AdaBoostRegressor(base_estimator=base_estimator, n_estimators=10, learning_rate=0.1)\n",
    "\n",
    "    params={\"max_depth\":3,\"learning_rate\":0.1,\"n_estimators\":10,\"silent\":0,\"objective\":\"reg:linear\"}\n",
    "    xgbr=xgb.XGBRegressor(**params)\n",
    "    model=xgb.XGBRegressor()\n",
    "    parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500]}\n",
    "    gsxgbr = GridSearchCV(model,parameters,cv = 5,n_jobs = 5,verbose=True)\n",
    "    return [bagr,rfr,dbr,adar,xgbr,gsxgbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def gen_svm():\n",
    "    svr_linear = svm.SVR(kernel='linear', C=100)\n",
    "    svr_poly = svm.SVR(kernel='poly', degree=3, C=100)\n",
    "    svr_rbf = svm.SVR(kernel='rbf', gamma=0.2, C=100)\n",
    "    return [svr_linear,svr_rbf,svr_poly]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "def gen_em():\n",
    "    gmm = GaussianMixture(n_components=1, covariance_type='full', random_state=0)\n",
    "    dpgmm = BayesianGaussianMixture(n_components=1, covariance_type='full', max_iter=1000, n_init=5,\n",
    "                                    weight_concentration_prior_type='dirichlet_process', weight_concentration_prior=0.1)\n",
    "    return [gmm,dpgmm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['OMP_NUM_THREADS'] = '4'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def df2tc(df):\n",
    "    features=torch.FloatTensor(df.values)\n",
    "    tc=torch.unsqueeze(features,dim=1)\n",
    "    tc=torch.autograd.Variable(tc)\n",
    "    return tc\n",
    "    \n",
    "def tc2df(tc):\n",
    "    n=tc.data.numpy()\n",
    "    return pd.DataFrame(n)\n",
    "    \n",
    "def fcnn(x_train,y_train,x_test):\n",
    "    print(\"开始构建神经网络\")\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_path=\"fcnn.model\"\n",
    "    # Hyper-parameters\n",
    "    input_size = len(x_train.iloc[0])\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.1\n",
    "\n",
    "#     x_train=torch.from_numpy(x_train.values)\n",
    "    \n",
    "#     y_train=torch.tensor(y_train)\n",
    "#     x_test=torch.tensor(x_test)\n",
    "    x_train_features=torch.FloatTensor(x_train.values)\n",
    "    x_train=torch.unsqueeze(x_train_features,dim=1)\n",
    "    x_train=torch.autograd.Variable(x_train)\n",
    "#     x_train,y_train,x_test=torch.autograd.Variable(x_train,y_train,x_test)\n",
    "#     x_train=torch.from_numpy(x_train.values.astype(np.float32))\n",
    "#     input_size = x_train.size(0)\n",
    "\n",
    "\n",
    "    # Fully connected neural network with one hidden layer\n",
    "    class NeuralNet(nn.Module):\n",
    "        def __init__(self,input_size):\n",
    "            super(NeuralNet, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                                        nn.Linear(input_size, 1024),\n",
    "                                        nn.ReLU(),                                        \n",
    "                                        nn.Linear(1024, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(1024, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(64, 1))\n",
    "\n",
    "        def forward(self, x):\n",
    "            # out = self.fc1(x)\n",
    "            # out = self.relu(out)\n",
    "            # out = self.fc2(out)\n",
    "            # return out\n",
    "            return self.model(x)\n",
    "        \n",
    "    net = NeuralNet(input_size).to(device)\n",
    "    print('神经网络为 {}'.format(net))\n",
    "\n",
    "    # Loss and optimizer\n",
    "    loss_func = torch.nn.MSELoss()      # 预测值和真实值的误差计算公式 (均方差)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)  # 传入 net 的所有参数, 学习率\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            out = net(x_train)  # 喂给 net 训练数据 x, 每迭代一步，输出预测值\n",
    "            loss = loss_func(out, y_train)  # 计算两者的误差\n",
    "            optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
    "            loss.backward()         # 误差反向传播, 计算参数更新值\n",
    "            optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n",
    "            if epoch % 1 == 0:\n",
    "                print('Epoch [{}/{}]  Loss: {:.4f}'  .format(epoch , num_epochs, loss.item()))\n",
    "                print(datetime.datetime.now())\n",
    "\n",
    "                # Save the model checkpoint\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    else:\n",
    "        net.load_state_dict(torch.load(model_path))\n",
    "        net.eval()\n",
    "\n",
    "    #模型預測\n",
    "    y_predicted = net(torch.from_numpy(x_test)).detach().numpy()\n",
    "    y_predicted=y_predicted.data.numpy()\n",
    "    y_predicted=pd.DataFrame(y_predicted)\n",
    "    return y_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始时间: \n",
      "2019-03-03 21:14:36.984825\n",
      " 5.190158843994141 秒完数据读入\n",
      "(8000, 2)\n",
      "(8000, 100)\n",
      "(8000, 1000)\n",
      "(8000, 1102)\n",
      " 1.2000365257263184 秒完成特征提取\n",
      "\n",
      "2019-03-03 21:14:43.422018\n",
      "开始构建神经网络\n",
      "神经网络为 NeuralNet(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1102, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [8000 x 64], m2: [1024 x 64] at c:\\a\\w\\1\\s\\tmp_conda_3.6_090826\\conda\\conda-bld\\pytorch_1550394668685\\work\\aten\\src\\th\\generic/THTensorMath.cpp:940",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-305-72cb06e012cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdeeps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfcnn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdeeps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mdl_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" 所有模型预测完毕  {0} 秒耗时总计\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-298-0d1db75ddafc>\u001b[0m in \u001b[0;36mdl_predict\u001b[1;34m(model, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0my_predict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0my_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrmsle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-304-f72dbb82dfbc>\u001b[0m in \u001b[0;36mfcnn\u001b[1;34m(x_train, y_train, x_test)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 喂给 net 训练数据 x, 每迭代一步，输出预测值\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 计算两者的误差\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 清空上一步的残余更新参数值\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-304-f72dbb82dfbc>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;31m# out = self.fc2(out)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# return out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1354\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [8000 x 64], m2: [1024 x 64] at c:\\a\\w\\1\\s\\tmp_conda_3.6_090826\\conda\\conda-bld\\pytorch_1550394668685\\work\\aten\\src\\th\\generic/THTensorMath.cpp:940"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    t0 = time()\n",
    "    t = t0\n",
    "    print(\"开始时间: \")\n",
    "    print(datetime.datetime.now())\n",
    "    dir = r\"input/\"\n",
    "    x_train,y_train,x_test,y_test=get_data(dir)\n",
    "\n",
    "    gens=[gen_linear,gen_tree,gen_svm,gen_ensem,gen_em]\n",
    "#     gens=[gen_linear]\n",
    "#     for gen in gens:\n",
    "#         ml_predict(gen,x_train,y_train,x_test,y_test)\n",
    "    deeps=[fcnn]\n",
    "    for model in deeps:\n",
    "        dl_predict(model,x_train,y_train,x_test,y_test)\n",
    "\n",
    "    print(\" 所有模型预测完毕  {0} 秒耗时总计\".format(time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    t0 = time()\n",
    "    t = t0\n",
    "    print(\"开始时间: \")\n",
    "    print(datetime.datetime.now())\n",
    "    dir = r\"input/\"\n",
    "    x_train,y_train,x_test,y_test=get_data(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    deeps=[fcnn]\n",
    "    for model in deeps:\n",
    "        dl_predict(model,x_train,y_train,x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
