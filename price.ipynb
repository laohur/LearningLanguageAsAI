{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Mercari Price Suggestion Challenge\n",
    "\n",
    "https://www.kaggle.com/c/mercari-price-suggestion-challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from time import time\n",
    "import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n",
    "#Source: https://www.kaggle.com/marknagelberg/rmsle-function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "def get_data(dir):\n",
    "    t=time()\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "    train=pd.read_csv(dir+\"train.tsv\",sep='\\t').fillna(\" \")\n",
    "    test=pd.read_csv(dir+\"test.tsv\",sep='\\t').fillna(\" \")\n",
    "    print(train.shape)\n",
    "    #print(train.head(3))\n",
    "#     print(train.describe())\n",
    "    print(test.shape)\n",
    "    #print(test.head(3))\n",
    "#     print(test.describe())\n",
    "\n",
    "\n",
    "#     train.loc[train['price'] <=0] = 1\n",
    "#     test.loc[test['price'] <=0] = 1\n",
    "#     print(train.describe())\n",
    "#     print(test.describe())\n",
    "    print(\" {0} 秒完数据读入\".format(time() - t))\n",
    " \n",
    "    t=time()\n",
    "    #此时归一化，tfidf会自私归一化，而且耗时很长\n",
    "    x_train=train[[\"item_condition_id\",\"shipping\"]]\n",
    "    x_test=test[[\"item_condition_id\",\"shipping\"]]\n",
    "    y_train=train[\"price\"]\n",
    "    y_test=test[\"price\"]\n",
    "    \n",
    "    x_train= pd.DataFrame(SimpleImputer ().fit_transform(x_train))\n",
    "#     y_train= Imputer().fit_transform(y_train)\n",
    "    x_test= pd.DataFrame(SimpleImputer ().fit_transform(x_test))\n",
    "#     y_test= Imputer().fit_transform(y_test)    \n",
    "    t = time()\n",
    "    print(x_train.shape)\n",
    "    #print(x_train.head(3))\n",
    "    \n",
    "#     x_train,x_test=add_tfidf(x_train,x_test,train,test)\n",
    "    x_train,x_test=add_word2vec(x_train,x_test,train,test)\n",
    "    x_train= pd.DataFrame(SimpleImputer ().fit_transform(x_train))\n",
    "#     y_train= Imputer().fit_transform(y_train)\n",
    "    x_test= pd.DataFrame(SimpleImputer ().fit_transform(x_test))\n",
    "#     y_test= Imputer().fit_transform(y_test)\n",
    "\n",
    "    x_train = pd.DataFrame(MinMaxScaler().fit_transform(x_train))\n",
    "    x_test = pd.DataFrame(MinMaxScaler().fit_transform(x_test))\n",
    "\n",
    "    # print(x_train.head(3))\n",
    "    print(x_train.shape)\n",
    "    # print(x_test.head(3))\n",
    "    print(x_test.shape)\n",
    "    print(\" {0} 秒完成特征提取\".format(time() - t))\n",
    "    t = time()\n",
    "\n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def add_tfidf(x_train,x_test,train,test):\n",
    "# 不可四列一起训练，name band 太稀少了  不划算\n",
    "    text_columns=[\"category_name\",\"brand_name\",\"name\",\"item_description\"]\n",
    "    max_features=[1000,10000,20000,100000]\n",
    "    max_features=[100,1000,2000,10000]\n",
    "    tfidfer=[None]*4\n",
    "    for i in range(4):\n",
    "        tfidfer[i]=TfidfVectorizer(max_features=max_features[i],ngram_range=(1,1)).fit(train[text_columns[i]])\n",
    "        a=tfidfer[i].transform(train[text_columns[i]])\n",
    "        b=pd.DataFrame(a.toarray(),columns=tfidfer[i].get_feature_names())\n",
    "        # print(tfidfer[i].get_feature_names())\n",
    "        print(b.shape)\n",
    "     #   print(b.head(3))\n",
    "        x_train=x_train.join(b,rsuffix=\"_%s_\" %(text_columns[i]))\n",
    "\n",
    "        c=tfidfer[i].transform(test[text_columns[i]])\n",
    "        d=pd.DataFrame(c.toarray(),columns=tfidfer[i].get_feature_names())\n",
    "        x_test=x_test.join(d,rsuffix=\"_%s_\"%(text_columns[i]))\n",
    "\n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def add_word2vec(x_train,x_test,train,test):\n",
    "# 不可四列一起训练，name band 太稀少了  不划算\n",
    "    text_columns=[\"category_name\",\"brand_name\",\"name\",\"item_description\"]\n",
    "    num_features=[50,50,100,100]\n",
    "    for i in range(4):\n",
    "        series=train[text_columns[i]].astype(str)\n",
    "        print(type(series))\n",
    "#         print(series.head(2))\n",
    "        lines=series.values.tolist()\n",
    "        print(type(lines[0]))\n",
    "#         print(lines[0:3])\n",
    "#             cutWords_list = [k.split() for k in file.readlines()]\n",
    "        sentences=[line.split() for line in lines]\n",
    "    #    sentences=train[\"item_description\"]\n",
    "    #    sentences=x_train[[\"name\",\"item_description\"]\n",
    "        print(type(sentences))\n",
    "        # 模型参数\n",
    "        num_feature = num_features[i]    # Word vector dimensionality                      \n",
    "        min_word_count = 40   # Minimum word count                        \n",
    "        num_workers = 4       # Number of threads to run in parallel\n",
    "        context = 10          # Context window size                                                                                    \n",
    "        downsampling = 1e-3   # Downsample setting for frequent words\n",
    "        model = Word2Vec(sentences, workers=num_workers, \\\n",
    "                    size=num_feature, min_count=min_word_count, \\\n",
    "                    window=context, sample=downsampling)\n",
    "#         model=Word2Vec(sentences,size=num_feature)\n",
    "        b=pd.DataFrame(getAvgFeatureVecs(sentences,model,num_feature))\n",
    "        \n",
    "        x_train=x_train.join(b,rsuffix=\"_%s_\" %(text_columns[i]))\n",
    "        \n",
    "        series=test[text_columns[i]].astype(str)\n",
    "        lines=series.values.tolist()\n",
    "        sentences=[line.split() for line in lines]\n",
    "\n",
    "        \n",
    "        d=pd.DataFrame(getAvgFeatureVecs(sentences,model,num_feature))\n",
    "        x_test=x_test.join(d,rsuffix=\"_%s_\"%(text_columns[i]))\n",
    "\n",
    "        print(x_train.shape)\n",
    "#         print(x_train.head(3))\n",
    "        print(x_test.shape)\n",
    "#         print(x_test.head(3))\n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(sentence, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in sentence:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(sentences, model, num_feature):\n",
    "    counter = 0\n",
    "    vecs = np.zeros((len(sentences), num_feature), dtype=\"float64\")\n",
    "\n",
    "    for sentence in sentences:\n",
    "#         if counter % 10000 == 0:\n",
    "#             print(\"sentence %d of %d\" % (counter, len(sentences)))\n",
    "\n",
    "        vecs[counter] = makeFeatureVec(sentence, model, num_feature)\n",
    "        counter += 1\n",
    "\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_predict(gen_models,x_train,y_train,x_test,y_test):\n",
    "\n",
    "    models=gen_models()\n",
    "    result=[[\"model\",\"rmsle\"],[\"time\"]]\n",
    "    for model in models:\n",
    "        t=time()\n",
    "        print()\n",
    "        print(datetime.datetime.now())        \n",
    "        model=model.fit(x_train,y_train)\n",
    "        y_predict=model.predict(x_test)\n",
    "        y_predict[y_predict<=0] = 1\n",
    "        error=rmsle(y_test,y_predict)\n",
    "        print(datetime.datetime.now())\n",
    "        print(model.__module__+\"训练结束\")\n",
    "        print(\"rmsle:%f\" %error)\n",
    "#         print(\"训练结束\"+model.get_params)\n",
    "        eclapsed=time() - t\n",
    "        print(\" {0} 秒耗时总计\".format(eclapsed))\n",
    "#         result.append([model.__name__,error,eclapsed])\n",
    "        result.append([\"name\",error,eclapsed])\n",
    "        \n",
    "#         import matplotlib as mpl\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         axis=np.arange(len(y_test))\n",
    "#         mpl.rcParams['font.sans-serif'] = ['simHei']\n",
    "#         mpl.rcParams['axes.unicode_minus'] = False\n",
    "#         plt.figure(facecolor='w')\n",
    "#         plt.plot(t, y_test, 'r-', lw=2, label='真实值')\n",
    "#         plt.plot(t, y_predict, 'g-', lw=2, label='估计值')\n",
    "#         plt.legend(loc='best')\n",
    "# #         plt.title(model.___name_, fontsize=18)\n",
    "#         plt.xlabel('商品编号', fontsize=15)\n",
    "#         plt.ylabel('商品价格', fontsize=15)\n",
    "#         plt.grid()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "模型\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "def gen_linear():\n",
    "    linear=LinearRegression()\n",
    "    ridge=Ridge()\n",
    "    lasso=Lasso()\n",
    "#     linear_pipeline=Pipeline([\n",
    "#             ('poly', PolynomialFeatures()),\n",
    "#             ('linear', ElasticNetCV(alphas=np.logspace(-3, 2, 10), l1_ratio=[.1, .5, .7, .9, .95, .99, 1],\n",
    "#                                     fit_intercept=False))])\n",
    "#     return [linear,ridge,lasso,linear_pipeline]\n",
    "    return [linear,ridge,lasso]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "def gen_tree():\n",
    "    dtr=DecisionTreeRegressor(criterion='mse', max_depth=5)\n",
    "    return [dtr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor,RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "def gen_ensem():\n",
    "    ridge = RidgeCV(alphas=np.logspace(-3, 2, 3), fit_intercept=False)\n",
    "    ridged = Pipeline([('poly', PolynomialFeatures(degree=degree)), ('Ridge', ridge)])\n",
    "    bagr= BaggingRegressor(ridged, n_estimators=n_estimators, max_samples=max_samples)\n",
    "    rfr = RandomForestRegressor(n_estimators=10, criterion='entropy', max_depth=5, oob_score=True)\n",
    "    dbr= GradientBoostingRegressor(n_estimators=10, learning_rate=0.1, max_depth=2)\n",
    "    adar = AdaBoostRegressor(base_estimator=base_estimator, n_estimators=10, learning_rate=0.1)\n",
    "\n",
    "    params={\"max_depth\":3,\"learning_rate\":0.1,\"n_estimators\":10,\"silent\":0,\"objective\":\"reg:linear\"}\n",
    "    xgbr=xgb.XGBRegressor(**params)\n",
    "    parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500]}\n",
    "    gsxgbr = GridSearchCV(model,parameters,cv = 5,n_jobs = 5,verbose=True)\n",
    "    return [bagr,rfr,dbr,adar,xgbr,gsxgbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def gen_svm():\n",
    "    svr_linear = svm.SVR(kernel='linear', C=100)\n",
    "    svr_poly = svm.SVR(kernel='poly', degree=3, C=100)\n",
    "    svr_rbf = svm.SVR(kernel='rbf', gamma=0.2, C=100)\n",
    "    return [svr_linear,svr_rbf,svr_poly]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "def gen_em():\n",
    "    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=0)\n",
    "    dpgmm = BayesianGaussianMixture(n_components=n_components, covariance_type='full', max_iter=1000, n_init=5,\n",
    "                                    weight_concentration_prior_type='dirichlet_process', weight_concentration_prior=0.1)\n",
    "    return [gmm,dpgmm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始时间: \n",
      "2019-03-02 22:18:41.463831\n",
      "(36689, 8)\n",
      "(9173, 8)\n",
      " 0.19148778915405273 秒完数据读入\n",
      "(36689, 2)\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'str'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36689, 52)\n",
      "(9173, 52)\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'str'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36689, 102)\n",
      "(9173, 102)\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'str'>\n",
      "<class 'list'>\n",
      "(36689, 202)\n",
      "(9173, 202)\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'str'>\n",
      "<class 'list'>\n",
      "(36689, 302)\n",
      "(9173, 302)\n",
      "(36689, 302)\n",
      "(9173, 302)\n",
      " 18.9547860622406 秒完成特征提取\n",
      "\n",
      "2019-03-02 22:19:00.623072\n",
      "2019-03-02 22:19:01.183588\n",
      "sklearn.linear_model.base训练结束\n",
      "rmsle:2.700371\n",
      " 0.561514139175415 秒耗时总计\n",
      "\n",
      "2019-03-02 22:19:01.184586\n",
      "2019-03-02 22:19:01.484467\n",
      "sklearn.linear_model.ridge训练结束\n",
      "rmsle:1.573226\n",
      " 0.2998814582824707 秒耗时总计\n",
      "\n",
      "2019-03-02 22:19:01.484467\n",
      "2019-03-02 22:19:02.032020\n",
      "sklearn.linear_model.coordinate_descent训练结束\n",
      "rmsle:0.943650\n",
      " 0.5475528240203857 秒耗时总计\n",
      "\n",
      "2019-03-02 22:19:02.032020\n",
      "2019-03-02 22:19:05.783668\n",
      "sklearn.tree.tree训练结束\n",
      "rmsle:0.807894\n",
      " 3.75164794921875 秒耗时总计\n",
      "\n",
      "2019-03-02 22:19:05.783668\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    t0 = time()\n",
    "    t = t0\n",
    "    print(\"开始时间: \")\n",
    "    print(datetime.datetime.now())\n",
    "    dir = r\"C:/Users/zen/code/Mercari Price Suggestion Challenge/data/\"\n",
    "    dir = r\"./data/\"\n",
    "    x_train,y_train,x_test,y_test=get_data(dir)\n",
    "    # x_train,y_train,x_test,y_test=get_word2vec(dir)\n",
    "    # x_train,y_train,x_test,y_test=get_brtt(dir)\n",
    "    #y_predict=xgb_predict(x_train,y_train,x_test)\n",
    "    #models: xgb linear rindge lasso decissiontree randomforest svr gmm \n",
    "    #models: linear tree svm em\n",
    "    gens=[gen_linear,gen_tree,gen_svm,gen_em]\n",
    "#     gens=[gen_linear]\n",
    "    for gen in gens:\n",
    "        model_predict(gen,x_train,y_train,x_test,y_test)\n",
    "    # y_predict=xgb_model(x_train,y_train,x_test)\n",
    "    # print(\"rmsle:%f\" %rmsle(y_test,y_predict))\n",
    "    # print(\" {0} 秒耗时总计\".format(time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
